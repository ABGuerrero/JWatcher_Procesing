{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9f2e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7857390",
   "metadata": {},
   "source": [
    "### Function that will normalize the way the date is written on the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8bcabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_date(date_str):\n",
    "    from datetime import datetime\n",
    "    try:\n",
    "        # Attempt to parse the date in various formats\n",
    "        date_formats = ['_%Y_%m_%d', '_%y_%m_%d', '_%y_%m_%d']\n",
    "        for fmt in date_formats:\n",
    "            try:\n",
    "                date_obj = datetime.strptime(date_str, fmt)\n",
    "                return date_obj.strftime('%Y-%m-%d')\n",
    "            except ValueError:\n",
    "                pass  # Continue to next format\n",
    "        # If none of the formats match, return None\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while parsing date: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed75e37",
   "metadata": {},
   "source": [
    "### Function that will read the .dat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecdac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dat_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Find the index of the line that says '#BEGIN DATA'\n",
    "    data_start_index = next(i for i, line in enumerate(lines) if '#BEGIN DATA' in line)\n",
    "    \n",
    "    # Initialize lists to store trial data\n",
    "    start_zones = []\n",
    "    end_zones = []\n",
    "    stops = []\n",
    "    prods = []\n",
    "    \n",
    "    # Process lines after '#BEGIN DATA'\n",
    "    i = data_start_index + 1\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip().split(',')\n",
    "        if len(line) < 2:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Parse the zone and value\n",
    "        zone = line[1].strip()\n",
    "        \n",
    "        if zone == 'EOF':\n",
    "            break\n",
    "        elif zone == 'n':\n",
    "            # Skip trial if it contains 'n'\n",
    "            i += 1\n",
    "            continue\n",
    "        elif zone == 'f':\n",
    "            # Confirm end of trial, move to next trial\n",
    "            i += 1\n",
    "            continue\n",
    "        elif zone.isdigit():\n",
    "            zone = int(zone)\n",
    "            if len(start_zones) == len(end_zones):\n",
    "                start_zones.append(zone)\n",
    "                stops.append(0)\n",
    "                prods.append(0)\n",
    "            else:\n",
    "                end_zones.append(zone)\n",
    "        elif zone == 's':\n",
    "            stops[-1] += 1\n",
    "        elif zone == 'p':\n",
    "            prods[-1] += 1\n",
    "        i += 1\n",
    "    \n",
    "    # Create a DataFrame from the collected data\n",
    "    trial_data = pd.DataFrame({\n",
    "        'Start_Zone': start_zones,\n",
    "        'End_Zone': end_zones,\n",
    "        'Stops': stops,\n",
    "        'Prods': prods\n",
    "    })\n",
    "    \n",
    "    return trial_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af951ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(read_dat_file(\"X:\\MATT_SCORING\\RMB4_2023_07_09.dat\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c31336",
   "metadata": {},
   "source": [
    "## Here we will input the rewarded zone.\n",
    "### With the function ErrorScore we will determine the difference between the end zone (second in each pair) and the Rewarded Zone (RewZone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169c8c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ErrorScore(data_path, RewZone):\n",
    "    errors = []\n",
    "    trial_data = read_dat_file(data_path)\n",
    "    for index, row in trial_data.iterrows():\n",
    "        start_zone = row['Start_Zone']\n",
    "        end_zone = row['End_Zone']\n",
    "        \n",
    "        if end_zone == RewZone:\n",
    "            error = 0\n",
    "        else:\n",
    "            error = (end_zone - RewZone) % 8\n",
    "            if error > 4:\n",
    "                error -= 8\n",
    "        \n",
    "        errors.append(error)\n",
    "    \n",
    "    trial_data['Error'] = errors\n",
    "    return trial_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8b27b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = ErrorScore(\"X:\\MATT_SCORING\\RMB4_2023_07_09.dat\", 5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054d0a0a",
   "metadata": {},
   "source": [
    "### Iteration function\n",
    "#### This function will iterate through a folder of your choice and look for .dat files whose name starts with the string you also input, which should be the ID of the animal. \n",
    "#### The function will create txt files that will have the vector created by ErrorScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb4564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_in_folder(folder_path, RewZone):\n",
    "    file_prefix = 'RMB'\n",
    "    # Find all .dat files starting with the given prefix in the specified folder\n",
    "    search_pattern = os.path.join(folder_path, f'{file_prefix}*.dat')\n",
    "    dat_files = glob.glob(search_pattern)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for dat_file in dat_files:\n",
    "        # Read data from the .dat file\n",
    "        trial_data = read_dat_file(dat_file)\n",
    "        \n",
    "        if not trial_data.empty:\n",
    "            # Calculate error scores and create DataFrame\n",
    "            df = ErrorScore(dat_file, RewZone)\n",
    "            \n",
    "            # Extract the date from the filename\n",
    "            base_name = os.path.basename(dat_file)\n",
    "            date_part = base_name.replace(file_prefix, '').replace('.dat', '')\n",
    "            normalized_day = normalize_date(date_part)\n",
    "            # Add Date column to DataFrame\n",
    "            df.insert(0, 'Date', normalized_day)\n",
    "            \n",
    "            # Append DataFrame to all_results\n",
    "            all_results.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames into one\n",
    "    result_df = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "    # Add Date and Trial indices\n",
    "    result_df['Date_Index'] = result_df.groupby('Date').ngroup() + 1\n",
    "    result_df['Trial_Index'] = result_df.groupby('Date').cumcount() + 1\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    output_filename = os.path.join(folder_path, f'MEA_Results_{os.path.basename(folder_path)}.csv')\n",
    "    result_df.to_csv(output_filename, index=False)\n",
    "    print(f\"Results saved to {output_filename}\")\n",
    "    \n",
    "    classify_errors(output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ec4737",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'X:\\MATT_SCORING'  # Replace with the path to your folder\n",
    "file_prefix = 'RMB4'  # Example file prefix to look for\n",
    "RewZone = 5  # Example value for RewZone\n",
    "process_files_in_folder(folder_path, file_prefix, RewZone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e296dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_absolute_error_with_sem(csv_file):\n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Drop rows with invalid dates\n",
    "    df = df.dropna(subset=['Date'])\n",
    "    \n",
    "    # Calculate the absolute error\n",
    "    df['Absolute Error'] = df['Error'].abs()\n",
    "    \n",
    "    # Group by date and calculate the mean absolute error and SEM\n",
    "    grouped = df.groupby('Date')['Absolute Error']\n",
    "    mean_absolute_error = grouped.mean().reset_index()\n",
    "    sem_absolute_error = grouped.sem().reset_index()\n",
    "    \n",
    "    # Sort by date in ascending order\n",
    "    mean_absolute_error = mean_absolute_error.sort_values(by='Date')\n",
    "    sem_absolute_error = sem_absolute_error.sort_values(by='Date')\n",
    "    \n",
    "    # Plot the mean absolute error with SEM as shaded ribbon\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    sns.lineplot(data=mean_absolute_error, x='Date', y='Absolute Error', marker='o')\n",
    "    plt.fill_between(mean_absolute_error['Date'], mean_absolute_error['Absolute Error'] - sem_absolute_error['Absolute Error'], mean_absolute_error['Absolute Error'] + sem_absolute_error['Absolute Error'], alpha=0.3)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.title('Mean Absolute Error by Date with SEM (Ribbon Plot)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd314f74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csv_file = \"X:\\MATT_SCORING\\MEA_Results_RMB4.csv\"  # Replace with the path to your CSV file\n",
    "plot_mean_absolute_error_with_sem(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f056bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_errors(csv_file):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Initialize an empty list to store error classifications\n",
    "    error_types = []\n",
    "    \n",
    "    # Initialize variables to keep track of the previous trial's data\n",
    "    prev_error = None\n",
    "    prev_motion = None\n",
    "    \n",
    "    # Iterate over rows in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # Current trial's data\n",
    "        current_error = row['Error']\n",
    "        current_motion = row['End_Zone'] - row['Start_Zone']\n",
    "        \n",
    "        # Handle the first row separately\n",
    "        if index == 0:\n",
    "            # Check if the first trial is correct\n",
    "            if current_error == 0:\n",
    "                error_type = 'Correct'\n",
    "            else:\n",
    "                error_type = 'NA'  # Not Applicable for the first row\n",
    "        else:\n",
    "            # Check if the current trial is correct\n",
    "            if current_error == 0:\n",
    "                error_type = 'Correct'\n",
    "            else:\n",
    "                # Check conditions for error classification\n",
    "                if prev_error == 0 and current_motion == prev_motion:\n",
    "                    error_type = 'ECS'  # Egocentric Centered Strategy\n",
    "                elif prev_error != 0 and current_motion == prev_motion:\n",
    "                    error_type = 'NFES'  # Non-flexible Egocentric Strategy\n",
    "                elif abs(current_error) == 1:\n",
    "                    error_type = 'IT'  # Inexact Transformation\n",
    "                else:\n",
    "                    error_type = 'RS'  # Random Strategy\n",
    "        \n",
    "        # Append error type to the list\n",
    "        error_types.append(error_type)\n",
    "        \n",
    "        # Update previous trial's data for the next iteration\n",
    "        prev_error = current_error\n",
    "        prev_motion = current_motion\n",
    "    \n",
    "    # Add the Error_Type column to the DataFrame\n",
    "    df['Error_Type'] = error_types\n",
    "    \n",
    "    # Save the DataFrame back to the CSV file\n",
    "    df.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f464c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"X:\\MATT_SCORING\\MEA_Results_RMB4.csv\"  # Replace with the path to your CSV file\n",
    "classify_errors(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a8abcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_frequency(csv_file):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Group the data by date and error type, and count the occurrences\n",
    "    error_counts = df.groupby(['Date', 'Error_Type']).size().unstack(fill_value=0)\n",
    "    error_counts = error_counts.drop(columns='Correct', errors='ignore')\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    # Plot each error type separately\n",
    "    for error_type in error_counts.columns:\n",
    "        plt.plot(error_counts.index, error_counts[error_type], label=error_type)\n",
    "\n",
    "    # Set plot labels and title\n",
    "    \n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Error Frequency by Date')\n",
    "    plt.legend(title='Error Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907dc418",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"X:\\MATT_SCORING\\RMB1_5\\MEA_Results_RMB1.csv\"  # Replace with the path to your CSV file\n",
    "plot_error_frequency(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e003a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stack_bar_error_frequency(csv_file):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Group the data by date and error type, and count the occurrences\n",
    "    error_counts = df.groupby(['Date', 'Error_Type']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Exclude the 'Correct' error type from the plot\n",
    "    error_counts = error_counts.drop(columns='Correct', errors='ignore')\n",
    "    \n",
    "    # Plot the frequency of each error type for each date\n",
    "    error_counts.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "    \n",
    "    # Set plot labels and title\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Error Frequency by Date')\n",
    "    plt.legend(title='Error Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c466cf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csv_file = \"X:\\MATT_SCORING\\RMB1_5\\MEA_Results_RMB1.csv\"  # Replace with the path to your CSV file\n",
    "plot_stack_bar_error_frequency(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c500d26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_percentage(csv_file):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Filter out the 'Correct' error type\n",
    "    df_incorrect = df[df['Error_Type'] != 'Correct']\n",
    "    \n",
    "    # Group the data by error type and count the occurrences\n",
    "    error_counts = df_incorrect['Error_Type'].value_counts()\n",
    "    \n",
    "    # Calculate the percentage of each error type relative to the total incorrect trials\n",
    "    error_percentage = error_counts / error_counts.sum() * 100\n",
    "    \n",
    "    # Define colors for each error type\n",
    "    error_colors = {'ECS': 'blue', 'IT': 'orange', 'NFES': 'green', 'RS': 'red'}\n",
    "    \n",
    "    # Plot the percentage of each error type with customized colors\n",
    "    error_percentage.plot(kind='bar', figsize=(10, 6), color=[error_colors.get(x, 'gray') for x in error_percentage.index])\n",
    "    \n",
    "    # Set plot labels and title\n",
    "    plt.xlabel('Error Type')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.title('Error Percentage Across All Dates (Incorrect Trials Only)')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00edfdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"X:\\MATT_SCORING\\RMB1_3\\MEA_Results_RMB1.csv\"  # Replace with the path to your CSV file\n",
    "plot_error_percentage(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e97f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"X:\\MATT_SCORING\\RMB1_5\\MEA_Results_RMB1.csv\"  # Replace with the path to your CSV file\n",
    "plot_error_percentage(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafd01d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"X:\\MATT_SCORING\\MEA_Results_RMB4.csv\"\n",
    "plot_error_percentage(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3616e713",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"X:\\MATT_SCORING\\MEA_Results_RMB2.csv\"\n",
    "plot_error_percentage(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0a210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_subfolders(root_folder):\n",
    "    subfolders = [f.path for f in os.scandir(root_folder) if f.is_dir()]\n",
    "    for subfolder in subfolders:\n",
    "        folder_name = os.path.basename(subfolder)\n",
    "        parts = folder_name.split('_')\n",
    "        if len(parts) == 2 and parts[1].isdigit():\n",
    "            animal_id, rew_zone = parts\n",
    "            rew_zone = int(rew_zone)\n",
    "            process_files_in_folder(subfolder, rew_zone)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320d4d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = 'X:/MATT_SCORING'  # Replace with the path to your folder\n",
    "process_all_subfolders(root_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c0591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import seaborn as sns\n",
    "\n",
    "def normalize_date(date_str):\n",
    "    from datetime import datetime\n",
    "    date_formats = ['_%Y_%m_%d', '_%y_%m_%d']\n",
    "    for fmt in date_formats:\n",
    "        try:\n",
    "            date_obj = datetime.strptime(date_str, fmt)\n",
    "            return date_obj.strftime('%Y-%m-%d')\n",
    "        except ValueError:\n",
    "            continue\n",
    "    print(f\"Failed to parse date: {date_str}\")\n",
    "    return None\n",
    "\n",
    "def extract_date_from_filename(filename):\n",
    "    parts = filename.split('_')\n",
    "    if len(parts) >= 3:\n",
    "        date_part = '_' + '_'.join(parts[-3:]).replace('.dat', '')\n",
    "        normalized_date = normalize_date(date_part)\n",
    "        return normalized_date\n",
    "    return None\n",
    "\n",
    "def read_dat_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    data_start_index = next(i for i, line in enumerate(lines) if '#BEGIN DATA' in line)\n",
    "    \n",
    "    start_zones = []\n",
    "    end_zones = []\n",
    "    stops = []\n",
    "    prods = []\n",
    "    \n",
    "    i = data_start_index + 1\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip().split(',')\n",
    "        if len(line) < 2:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        zone = line[1].strip()\n",
    "        \n",
    "        if zone == 'EOF':\n",
    "            break\n",
    "        elif zone == 'n':\n",
    "            i += 1\n",
    "            continue\n",
    "        elif zone == 'f':\n",
    "            i += 1\n",
    "            continue\n",
    "        elif zone.isdigit():\n",
    "            zone = int(zone)\n",
    "            if len(start_zones) == len(end_zones):\n",
    "                start_zones.append(zone)\n",
    "                stops.append(0)\n",
    "                prods.append(0)\n",
    "            else:\n",
    "                end_zones.append(zone)\n",
    "        elif zone == 's':\n",
    "            stops[-1] += 1\n",
    "        elif zone == 'p':\n",
    "            prods[-1] += 1\n",
    "        i += 1\n",
    "    \n",
    "    min_length = min(len(start_zones), len(end_zones), len(stops), len(prods))\n",
    "    start_zones = start_zones[:min_length]\n",
    "    end_zones = end_zones[:min_length]\n",
    "    stops = stops[:min_length]\n",
    "    prods = prods[:min_length]\n",
    "    \n",
    "    trial_data = pd.DataFrame({\n",
    "        'Start_Zone': start_zones,\n",
    "        'End_Zone': end_zones,\n",
    "        'Stops': stops,\n",
    "        'Prods': prods\n",
    "    })\n",
    "    \n",
    "    return trial_data\n",
    "\n",
    "def ErrorScore(data_path, RewZone):\n",
    "    errors = []\n",
    "    trial_data = read_dat_file(data_path)\n",
    "    for index, row in trial_data.iterrows():\n",
    "        start_zone = row['Start_Zone']\n",
    "        end_zone = row['End_Zone']\n",
    "        \n",
    "        if end_zone == RewZone:\n",
    "            error = 0\n",
    "        else:\n",
    "            error = (end_zone - RewZone) % 8\n",
    "            if error > 4:\n",
    "                error -= 8\n",
    "        \n",
    "        errors.append(error)\n",
    "    \n",
    "    trial_data['Error'] = errors\n",
    "    return trial_data\n",
    "\n",
    "def process_files_in_folder(folder_path, RewZone):\n",
    "    file_prefix = 'RMB'\n",
    "    search_pattern = os.path.join(folder_path, f'{file_prefix}*.dat')\n",
    "    dat_files = glob.glob(search_pattern)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for dat_file in dat_files:\n",
    "        trial_data = read_dat_file(dat_file)\n",
    "        \n",
    "        if not trial_data.empty:\n",
    "            df = ErrorScore(dat_file, RewZone)\n",
    "            \n",
    "            base_name = os.path.basename(dat_file)\n",
    "            normalized_day = extract_date_from_filename(base_name)\n",
    "\n",
    "            if normalized_day:\n",
    "                df.insert(0, 'Date', normalized_day)\n",
    "            \n",
    "                all_results.append(df)\n",
    "\n",
    "    if all_results:\n",
    "        result_df = pd.concat(all_results, ignore_index=True)\n",
    "    \n",
    "        # Index the date as 1 through whatever number\n",
    "        result_df['Date_Index'] = result_df.groupby('Date').ngroup() + 1\n",
    "        \n",
    "        # Index the trials within each date from 1 to whatever number\n",
    "        result_df['Trial_Index'] = result_df.groupby('Date').cumcount() + 1\n",
    "    \n",
    "        output_filename = os.path.join(folder_path, f'MEA_Results_{os.path.basename(folder_path)}.csv')\n",
    "        result_df.to_csv(output_filename, index=False)\n",
    "        print(f\"Results saved to {output_filename}\")\n",
    "        \n",
    "        classify_errors(output_filename)\n",
    "\n",
    "def classify_errors(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    error_types = []\n",
    "    \n",
    "    prev_error = None\n",
    "    prev_motion = None\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        current_error = row['Error']\n",
    "        current_motion = row['End_Zone'] - row['Start_Zone']\n",
    "        \n",
    "        if index == 0:\n",
    "            if current_error == 0:\n",
    "                error_type = 'Correct'\n",
    "            else:\n",
    "                error_type = 'NA'\n",
    "        else:\n",
    "            if current_error == 0:\n",
    "                error_type = 'Correct'\n",
    "            else:\n",
    "                if prev_error == 0 and current_motion == prev_motion:\n",
    "                    error_type = 'ECS'\n",
    "                elif prev_error != 0 and current_motion == prev_motion:\n",
    "                    error_type = 'NFES'\n",
    "                elif abs(current_error) == 1:\n",
    "                    error_type = 'IT'\n",
    "                else:\n",
    "                    error_type = 'RS'\n",
    "        \n",
    "        error_types.append(error_type)\n",
    "        \n",
    "        prev_error = current_error\n",
    "        prev_motion = current_motion\n",
    "    \n",
    "    df['Error_Type'] = error_types\n",
    "    \n",
    "    df.to_csv(csv_file, index=False)\n",
    "\n",
    "def process_all_subfolders(root_folder):\n",
    "    subfolders = [f.path for f in os.scandir(root_folder) if f.is_dir()]\n",
    "    for subfolder in subfolders:\n",
    "        folder_name = os.path.basename(subfolder)\n",
    "        parts = folder_name.split('_')\n",
    "        if len(parts) == 2 and parts[1].isdigit():\n",
    "            animal_id, rew_zone = parts\n",
    "            rew_zone = int(rew_zone)\n",
    "            process_files_in_folder(subfolder, rew_zone)\n",
    "\n",
    "root_folder = 'X:/MATT_SCORING'  # Replace with the path to your folder\n",
    "process_all_subfolders(root_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af628ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_csv_files(root_folder):\n",
    "    all_data = []\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                df = pd.read_csv(file_path)\n",
    "                animal_id = os.path.basename(subdir).split('_')[0]\n",
    "                df['Animal_ID'] = animal_id\n",
    "                all_data.append(df)\n",
    "    \n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        combined_filename = os.path.join(root_folder, 'combined_data.csv')\n",
    "        combined_df.to_csv(combined_filename, index=False)\n",
    "        print(f\"Combined data saved to {combined_filename}\")\n",
    "    else:\n",
    "        print(\"No CSV files found in the specified folder.\")\n",
    "\n",
    "root_folder = 'X:/MATT_SCORING'  # Replace with the root path to your folder containing all the CSVs\n",
    "combine_csv_files(root_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f994ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Load the combined dataset\n",
    "combined_data = pd.read_csv(\"X:\\MATT_SCORING\\combined_data.csv\")\n",
    "\n",
    "# Convert Animal_ID, Age, and Group to categorical data type\n",
    "combined_data['Animal_ID'] = combined_data['Animal_ID'].astype('category')\n",
    "combined_data['Age'] = combined_data['Age'].astype('category')\n",
    "combined_data['Group'] = combined_data['Group'].astype('category')\n",
    "\n",
    "# Define the formula for the linear mixed model\n",
    "formula = 'Error ~ C(Age) * C(Group)'\n",
    "\n",
    "# Fit the linear mixed model\n",
    "model = smf.mixedlm(formula, combined_data, groups=combined_data['Animal_ID'])\n",
    "result = model.fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e9406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of errors for each Error_Type within each ID and Age\n",
    "error_counts = combined_data.groupby(['Animal_ID', 'Age', 'Error_Type', 'Group']).size().reset_index(name='Count')\n",
    "\n",
    "# Calculate the mean and SEM across groups for each Age\n",
    "summary_df = error_counts.groupby(['Age', 'Group', 'Error_Type']).agg({'Count': ['mean', 'sem']}).reset_index()\n",
    "summary_df.columns = ['Age','Group', 'Error_Type', 'Mean', 'SEM']\n",
    "\n",
    "age_categories = {6: '6 months', 9: '9 months'}\n",
    "\n",
    "# Define functions for age category and order\n",
    "def age_to_category(age):\n",
    "    return age_categories.get(age, 'Other')  # Handle missing ages\n",
    "\n",
    "summary_df['Age_Group'] = summary_df['Age'].apply(age_to_category)\n",
    "summary_df['Age_Group'] = summary_df['Age_Group'].astype(str)  # Ensure string type\n",
    "\n",
    "def get_age_group_order(df):\n",
    "    # Get unique categories in the desired order (adjust order as needed)\n",
    "    return ['6 months', '9 months', 'Other']\n",
    "\n",
    "# Create the bar plot with error bars and faceting by Group\n",
    "age_group_order = get_age_group_order(summary_df)\n",
    "g = sns.FacetGrid(summary_df, col='Group', hue='Error_Type', col_wrap=2, palette='colorblind')\n",
    "g.map(sns.barplot, x='Age_Group', y='Mean', ci='sem', order=age_group_order)\n",
    "\n",
    "# Add labels and title\n",
    "g.fig.suptitle('Mean Number of Errors by Age, Group, and Error Type', fontsize=12)  # Set suptitle for all subplots\n",
    "g.fig.subplots_adjust(top=0.88)  # Adjust spacing between title and subplots\n",
    "\n",
    "# Rotate x-axis labels for better readability if needed\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba9b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_df['Age_Group'].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e3aa05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
